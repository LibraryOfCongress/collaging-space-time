{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Evaluations**\n",
    "### **Objectives**\n",
    "\n",
    "This project involves a complete script/notebook that processes a set of sample images for object detection, annotating each image with bounding boxes, labels, and confidence scores for all detected objects. The project includes the following key components:\n",
    "* *Image Processing:* The script reads our set of sample images to be used for object detection.\n",
    "    * Note: For now the script runs on an experimental sample set, but more to come...\n",
    "\n",
    "* *Object Detection:* Multiple pre-trained models are utilized to detect objects within the images.\n",
    "\n",
    "    * Faster R-CNN\n",
    "    * FCOS\n",
    "    * RetinaNet\n",
    "    * SSD\n",
    "    * SSDlite\n",
    "\n",
    "* *Visualization:* The results of the object detection are visualized with a side-by-side comparison of the annotated images from each model. Comparisions look at the top 5 predictions from each model and their respective bounding boxes on the same image.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Imports**\n",
    "\n",
    "Note: this is a lot imports. If this is the first time that you running the notebook, it may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Utilities\n",
    "import os\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from PIL import ImageFont, ImageDraw\n",
    "from IPython.display import display\n",
    "import regex as re\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision\n",
    "import time\n",
    "\n",
    "\n",
    "# Importing the Models and their respective weights\n",
    "from torchvision.models.detection import (\n",
    "    # Faster R-CNN\n",
    "    fasterrcnn_resnet50_fpn_v2,\n",
    "    FasterRCNN_ResNet50_FPN_V2_Weights,\n",
    "    # # FCOS\n",
    "    fcos_resnet50_fpn,\n",
    "    FCOS_ResNet50_FPN_Weights,\n",
    "    # RetinaNet\n",
    "    retinanet_resnet50_fpn_v2,\n",
    "    RetinaNet_ResNet50_FPN_V2_Weights,\n",
    "    # SSD\n",
    "    ssd300_vgg16,\n",
    "    SSD300_VGG16_Weights,\n",
    "    # SSDlite\n",
    "    ssdlite320_mobilenet_v3_large,\n",
    "    SSDLite320_MobileNet_V3_Large_Weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **output_image function**\n",
    "This function creates an output directory and subdirectories. In addition, it saves the images to a corresponding model subdirectories using the following:\n",
    "* *output_path*: The main directory where you want to save the output image.\n",
    "* *model_name*: name of the model, used to create a subdirectory. \n",
    "* *image_path*: original image path. The function extracts the base name of the image.\n",
    "* *im*: Model modified or unmodified PIL Image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_image(output_path, model_name, image_path, im):\n",
    "    # creating the general output dirctory\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    # creating the model's output directory:\n",
    "    model_output_directory = os.path.join(output_path, model_name)\n",
    "    if not os.path.exists(model_output_directory):\n",
    "        os.makedirs(model_output_directory)\n",
    "    # extracting the image base name:\n",
    "    base_name = os.path.basename(image_path)\n",
    "\n",
    "    # creating the full image output file name\n",
    "    output_path = os.path.join(model_output_directory, base_name)\n",
    "    print(f\"Model Output saved to {output_path}\")\n",
    "    im.save(output_path, \"JPEG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **object_detection function**\n",
    "This function generates the input model, processes the input image using model weights, and outputs the a visualization of the model's prediction (as annotated bounding boxes) to an output path.\n",
    "* *model*: model from *torchvision.models.detection*. This should already be imported.\n",
    "* *weights*: weights that correspond to model. Review torchvision model documentation.\n",
    "* *image_path*: original image path.\n",
    "* *output_path*: The main directory where you want to save the output image. DEFAULT= \"evaluation/outputs\".\n",
    "* *threshold*: model probability score threshold. DEFAULT= 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_detection(\n",
    "    model, weights, image_path, output_path=\"evaluation/outputs\", threshold=0.90\n",
    "):\n",
    "    #timer to see what parts of function take longer
    "    start = time.time()
    "    img = read_image(image_path)\n",
    "    # Step 1: Initialize model with the best available weights\n",
    "    weights = weights.DEFAULT\n",
    "    model_name = model.__name__\n",
    "\n",
    "    model = model(weights=weights)\n",
    "    model.eval()\n",
    "\n",
    "    # Step 2: Initialize the inference transforms\n",
    "    preprocess = weights.transforms()\n",
    "\n",
    "    # Step 3: Apply inference preprocessing transforms\n",
    "    batch = [preprocess(img)]\n",
    "\n",
    "    # Step 4: Use the model and visualize the prediction\n",
    "    prediction = model(batch)[0]\n",
    "\n",
    "    # Extracting the len of Index of the scores that meet the threshold value:\n",
    "    score_len = (prediction[\"scores\"] >= threshold).sum().item()\n",
    "    # Limits the scores at the threshold to just the top 5\n",
    "    if score_len >= 5:\n",
    "        score_len = 5\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print("How long it took to set up model: " + str(time.time() - start))
    "    start = time.time()
    "    # Step 5: Visualizing Model predictions onto image.\n",
    "\n",
    "    # Annotation features\n",
    "    image_heading = f\"Evaluating Top Detections\\n{model_name}: {score_len} detections at {threshold}\"\n",
    "    fill = (57, 255, 20)\n",
    "    font_path = os.path.abspath(\"fonts/OpenSans-Regular.ttf\")\n",
    "    font = ImageFont.truetype(font_path, 30)\n",
    "\n",
    "    if score_len == 0:  # If there are no predictions at the threshold, just o\n",
    "        im = to_pil_image(img)\n",
    "        draw = ImageDraw.Draw(im)\n",
    "        draw.text((5, 5), image_heading, font=font, fill=fill, align=\"left\")\n",
    "\n",
    "    else:  # If there are predictions, generate annotated visualization and save to corresponding output directory\n",
    "        labels = [\n",
    "            weights.meta[\"categories\"][i] for i in prediction[\"labels\"][:score_len]\n",
    "        ]\n",
    "        scores = prediction[\"scores\"][:score_len]\n",
    "        labels_with_scores = [\n",
    "            f\"{label} {score:.2f}\" for label, score in zip(labels, scores)\n",
    "        ]\n",
    "\n",
    "        box = draw_bounding_boxes(\n",
    "            img,\n",
    "            boxes=prediction[\"boxes\"][:score_len],\n",
    "            labels=labels_with_scores,\n",
    "            colors=\"red\",\n",
    "            width=4,\n",
    "            font=font_path,\n",
    "            font_size=20,\n",
    "        )\n",
    "        im = to_pil_image(box.detach())\n",
    "\n",
    "        # Draw the model name at the top left of the image\n",
    "        draw = ImageDraw.Draw(im)\n",
    "        draw.text((5, 5), image_heading, font=font, fill=fill, align=\"left\")\n",
    "\n",
    "    output_image(output_path, model_name, image_path, im)"
    "    print("At end of method: " + str(time.time()-start))
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **extract_number function**\n",
    "This function returns a sorted list of the files in a directory by extracting the number in the file names.\n",
    "* *directory_name*: name of directory with the files you need sorted. Note: this only works if the files already have some unique numerical ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number(directory_name):\n",
    "    match = re.findall(r\"\\d+\", directory_name)\n",
    "    # if match:\n",
    "    return int(match[0])\n",
    "    # return float(\"inf\")\n",
    "\n",
    "# Creating a sorted list of the filenames from my images dir\n",
    "sorted_filenames = sorted(os.listdir(\"images\"), key=extract_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code goes through the sorted image file names and saves annotated images of the predictions accross each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output saved to evaluation/outputs/fasterrcnn_resnet50_fpn_v2/image_22.jpg\n",
      "Model Output saved to evaluation/outputs/fcos_resnet50_fpn/image_22.jpg\n",
      "Model Output saved to evaluation/outputs/retinanet_resnet50_fpn_v2/image_22.jpg\n",
      "Model Output saved to evaluation/outputs/ssd300_vgg16/image_22.jpg\n",
      "Model Output saved to evaluation/outputs/ssdlite320_mobilenet_v3_large/image_22.jpg\n",
      "Model Output saved to evaluation/outputs/fasterrcnn_resnet50_fpn_v2/image_23.jpg\n",
      "Model Output saved to evaluation/outputs/fcos_resnet50_fpn/image_23.jpg\n",
      "Model Output saved to evaluation/outputs/retinanet_resnet50_fpn_v2/image_23.jpg\n",
      "Model Output saved to evaluation/outputs/ssd300_vgg16/image_23.jpg\n",
      "Model Output saved to evaluation/outputs/ssdlite320_mobilenet_v3_large/image_23.jpg\n",
      "Model Output saved to evaluation/outputs/fasterrcnn_resnet50_fpn_v2/image_24.jpg\n",
      "Model Output saved to evaluation/outputs/fcos_resnet50_fpn/image_24.jpg\n",
      "Model Output saved to evaluation/outputs/retinanet_resnet50_fpn_v2/image_24.jpg\n",
      "Model Output saved to evaluation/outputs/ssd300_vgg16/image_24.jpg\n",
      "Model Output saved to evaluation/outputs/ssdlite320_mobilenet_v3_large/image_24.jpg\n",
      "Model Output saved to evaluation/outputs/fasterrcnn_resnet50_fpn_v2/image_25.jpg\n",
      "Model Output saved to evaluation/outputs/fcos_resnet50_fpn/image_25.jpg\n",
      "Model Output saved to evaluation/outputs/retinanet_resnet50_fpn_v2/image_25.jpg\n",
      "Model Output saved to evaluation/outputs/ssd300_vgg16/image_25.jpg\n",
      "Model Output saved to evaluation/outputs/ssdlite320_mobilenet_v3_large/image_25.jpg\n",
      "Model Output saved to evaluation/outputs/fasterrcnn_resnet50_fpn_v2/image_26.jpg\n",
      "Model Output saved to evaluation/outputs/fcos_resnet50_fpn/image_26.jpg\n",
      "Model Output saved to evaluation/outputs/retinanet_resnet50_fpn_v2/image_26.jpg\n",
      "Model Output saved to evaluation/outputs/ssd300_vgg16/image_26.jpg\n",
      "Model Output saved to evaluation/outputs/ssdlite320_mobilenet_v3_large/image_26.jpg\n",
      "Model Output saved to evaluation/outputs/fasterrcnn_resnet50_fpn_v2/image_27.jpg\n",
      "Model Output saved to evaluation/outputs/fcos_resnet50_fpn/image_27.jpg\n",
      "Model Output saved to evaluation/outputs/retinanet_resnet50_fpn_v2/image_27.jpg\n",
      "Model Output saved to evaluation/outputs/ssd300_vgg16/image_27.jpg\n",
      "Model Output saved to evaluation/outputs/ssdlite320_mobilenet_v3_large/image_27.jpg\n",
      "Model Output saved to evaluation/outputs/fasterrcnn_resnet50_fpn_v2/image_28.jpg\n",
      "Model Output saved to evaluation/outputs/fcos_resnet50_fpn/image_28.jpg\n",
      "Model Output saved to evaluation/outputs/retinanet_resnet50_fpn_v2/image_28.jpg\n",
      "Model Output saved to evaluation/outputs/ssd300_vgg16/image_28.jpg\n",
      "Model Output saved to evaluation/outputs/ssdlite320_mobilenet_v3_large/image_28.jpg\n",
      "Model Output saved to evaluation/outputs/fasterrcnn_resnet50_fpn_v2/image_29.jpg\n",
      "Model Output saved to evaluation/outputs/fcos_resnet50_fpn/image_29.jpg\n",
      "Model Output saved to evaluation/outputs/retinanet_resnet50_fpn_v2/image_29.jpg\n",
      "Model Output saved to evaluation/outputs/ssd300_vgg16/image_29.jpg\n",
      "Model Output saved to evaluation/outputs/ssdlite320_mobilenet_v3_large/image_29.jpg\n",
      "Model Output saved to evaluation/outputs/fasterrcnn_resnet50_fpn_v2/image_30.jpg\n",
      "Model Output saved to evaluation/outputs/fcos_resnet50_fpn/image_30.jpg\n",
      "Model Output saved to evaluation/outputs/retinanet_resnet50_fpn_v2/image_30.jpg\n",
      "Model Output saved to evaluation/outputs/ssd300_vgg16/image_30.jpg\n",
      "Model Output saved to evaluation/outputs/ssdlite320_mobilenet_v3_large/image_30.jpg\n"
     ]
    }
   ],
   "source": [
    "for image in sorted_filenames[21:30]:\n",
    "    path = \"images/\" + image\n",
    "\n",
    "    object_detection(\n",
    "        fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, path\n",
    "    )\n",
    "    object_detection(fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, path)\n",
    "\n",
    "    object_detection(retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, path)\n",
    "\n",
    "    object_detection(ssd300_vgg16, SSD300_VGG16_Weights, path)\n",
    "\n",
    "    object_detection(\n",
    "        ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, path\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this code generates and saves a grid comparision image of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Comparison saved as evaluation/model_comparisons/images_1.jpg\n",
      "Model Comparison saved as evaluation/model_comparisons/images_2.jpg\n",
      "Model Comparison saved as evaluation/model_comparisons/images_3.jpg\n",
      "Model Comparison saved as evaluation/model_comparisons/images_4.jpg\n",
      "Model Comparison saved as evaluation/model_comparisons/images_5.jpg\n",
      "Model Comparison saved as evaluation/model_comparisons/images_6.jpg\n",
      "Model Comparison saved as evaluation/model_comparisons/images_7.jpg\n",
      "Model Comparison saved as evaluation/model_comparisons/images_8.jpg\n",
      "Model Comparison saved as evaluation/model_comparisons/images_9.jpg\n"
     ]
    }
   ],
   "source": [
    "# read images from computer\n",
    "eval_outputs_dir = 'evaluation/outputs/'\n",
    "\n",
    "folders = os.listdir(eval_outputs_dir)\n",
    "# image\n",
    "val = 1\n",
    "grid_number = len(os.listdir(eval_outputs_dir +\"fasterrcnn_resnet50_fpn_v2\"))\n",
    "for grid_image in range(grid_number):\n",
    "    image_vars = []\n",
    "    for i, folder in enumerate(os.listdir(eval_outputs_dir)):\n",
    "        sub_name = eval_outputs_dir + folder\n",
    "        sub_directory = os.listdir(sub_name)\n",
    "        # print(sub_directory[0])\n",
    "        image_vars.append(read_image(sub_name + \"/\" + sub_directory[grid_image]))\n",
    "\n",
    "    Grid = make_grid(image_vars)\n",
    "    img = torchvision.transforms.ToPILImage()(Grid)\n",
    "\n",
    "    comparison_dir = 'evaluation/' + 'model_comparisons'\n",
    "    if not os.path.exists(comparison_dir):\n",
    "        os.makedirs(comparison_dir)\n",
    "\n",
    "    panorama_name = comparison_dir + \"/images_\" + str(val) + \".jpg\"\n",
    "    img.save(panorama_name, \"JPEG\")\n",
    "    print(f\"Model Comparison saved as {panorama_name}\")\n",
    "    val += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LOC)",
   "language": "python",
   "name": "loc-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
