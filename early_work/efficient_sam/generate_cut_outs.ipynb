{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "import os\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from PIL import ImageFont, ImageDraw\n",
    "from IPython.display import display\n",
    "import regex as re\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.ndimage import binary_dilation, binary_erosion, binary_closing\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "\n",
    "from utils import show_mask, show_points, show_box, show_anns_ours, run_ours_box_or_points\n",
    "\n",
    "\n",
    "# Importing the Models and their respective weights\n",
    "from torchvision.models.detection import (\n",
    "    # Faster R-CNN\n",
    "    fasterrcnn_resnet50_fpn_v2,\n",
    "    FasterRCNN_ResNet50_FPN_V2_Weights,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientSam(\n",
       "  (image_encoder): ImageEncoderViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (invalid_points): Embedding(1, 256)\n",
       "    (point_embeddings): Embedding(1, 256)\n",
       "    (bbox_top_left_embeddings): Embedding(1, 256)\n",
       "    (bbox_bottom_right_embeddings): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): AttentionForTwoWayAttentionBlock(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): AttentionForTwoWayAttentionBlock(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (layers): ModuleList(\n",
       "              (0): Sequential(\n",
       "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "            )\n",
       "            (fc): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): AttentionForTwoWayAttentionBlock(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): AttentionForTwoWayAttentionBlock(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (final_output_upscaling_layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "        (2): GELU(approximate='none')\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): Identity()\n",
       "        (2): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x MLPBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Sequential(\n",
       "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (fc): Linear(in_features=256, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLPBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Sequential(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=256, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"EfficientSAM\")\n",
    "from efficient_sam.build_efficient_sam import build_efficient_sam_vitt, build_efficient_sam_vits\n",
    "import zipfile\n",
    "\n",
    "efficient_sam_vitt_model = build_efficient_sam_vitt()\n",
    "efficient_sam_vitt_model.eval()\n",
    "\n",
    "# Since EfficientSAM-S checkpoint file is >100MB, we store the zip file.\n",
    "with zipfile.ZipFile(\"weights/efficient_sam_vits.pt.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"weights\")\n",
    "efficient_sam_vits_model = build_efficient_sam_vits()\n",
    "efficient_sam_vits_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(parent_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\n",
    "model.eval()\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(path_img, structuring_value=25):\n",
    "    # Read the image\n",
    "    img = read_image(path_img)\n",
    "\n",
    "    batch = [preprocess(img)]\n",
    "    # Get prediction from the model\n",
    "    prediction = model(batch)[0]\n",
    "    \n",
    "    if len(prediction['labels']) == 0:\n",
    "        print('No Object Detection predictions within the Scope of MS COCO dataset')\n",
    "\n",
    "    else:\n",
    "    # Previously, i had incorporated a way to select the object for extracion\n",
    "    # dict_items = {}\n",
    "    # for i in range(len(prediction['labels'])):\n",
    "    #     dict_items[i] = weights.meta['categories'][prediction['labels'][i].item()]\n",
    "    #     # print(weights.meta['categories'][prediction['labels'][i].item()] + f':{i}')\n",
    "    # print(dict_items)\n",
    "    # print('input item # from dict of interest')\n",
    "    # x = int(input()) \n",
    "    # ignoring the input function and just setting x to 0 to show the top result, if exists.\n",
    "        name = os.path.basename(path_img).split('.')[0]\n",
    "        # print('generating mask for first object in: ' + name)\n",
    "        x = 0\n",
    "                \n",
    "\n",
    "        # Extract bounding box coordinates\n",
    "        x1 = prediction['boxes'][x][0].item()\n",
    "        y1 = prediction['boxes'][x][1].item()\n",
    "        x2 = prediction['boxes'][x][2].item()\n",
    "        y2 = prediction['boxes'][x][3].item()\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1\n",
    "\n",
    "\n",
    "        \n",
    "        # fig, ax = plt.subplots(1, 3, figsize=(30, 30))\n",
    "        input_point = np.array([[x1, y1], [x2, y2]])\n",
    "        input_label = np.array([2, 3])\n",
    "        \n",
    "\n",
    "        mask_efficient_sam_vitt = run_ours_box_or_points(path_img, input_point, input_label, efficient_sam_vitt_model)\n",
    "        # show_anns_ours(mask_efficient_sam_vitt, ax[1])\n",
    "        binary_mask = mask_efficient_sam_vitt\n",
    "        structuring_element = np.ones((structuring_value,structuring_value), dtype=bool)\n",
    "        dilated_mask = binary_dilation(binary_mask, structure=structuring_element)\n",
    "        eroded_mask = binary_erosion(dilated_mask, structure=structuring_element)\n",
    "\n",
    "        closed_mask_uint8 = (eroded_mask * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "\n",
    "        cv2.imwrite(f'masks/closed_{name}.png', closed_mask_uint8)\n",
    "        img_val = cv2.imread(path_img) \n",
    "        mask = cv2.imread(f'masks/closed_{name}.png')\n",
    "        img_foreground = np.array((mask/255)*(img_val/255)) * img_val\n",
    "        na = img_foreground\n",
    "        \n",
    "        '''\n",
    "        Import to note that part of the following code is from substack\n",
    "        '''\n",
    "        \n",
    "        # Make a True/False mask of pixels whose BGR values sum to more than zero\n",
    "        alpha = np.sum(na, axis=-1) > 0\n",
    "\n",
    "        # Convert True/False to 0/255 and change type to \"uint8\" to match \"na\"\n",
    "        alpha = np.uint8(alpha * 255)\n",
    "\n",
    "        # Stack new alpha layer with existing image to go from BGR to BGRA, i.e. 3 channels to 4 channels\n",
    "        res = np.dstack((na, alpha))\n",
    "        img = Image.fromarray(res, mode='RGBa')\n",
    "\n",
    "        # Save result\n",
    "        cv2.imwrite(f'cutouts/extraction_{name}.png', res)\n",
    "        img = Image.open(f'cutouts/extraction_{name}.png')\n",
    "        box = (x1-20, y1-20, x2+20, y2+30)\n",
    "        img = img.crop(box)\n",
    "        img.save(f'cutouts/extraction_{name}.png','PNG')\n",
    "\n",
    "        # cv2.imwrite(f'cutouts/extraction_{name}.png', img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_image(f'../images/image_2017878880.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Object Detection predictions within the Scope of MS COCO dataset\n"
     ]
    }
   ],
   "source": [
    "for image in os.listdir('../images')[25:26]:\n",
    "    path = \"../images/\" + image\n",
    "    # print(path)\n",
    "    process_image(path,30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LOC)",
   "language": "python",
   "name": "loc-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
