{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combination of the object detection and segmentation. Would output the masks and the 'cutouts'\n",
    "\n",
    "* Original image as a thumbnail -- (480 on the longest side as jpegs) **[DONE]**\n",
    "* Output the individual cutouts as png (include, label in filename: image_{resource_id}_{object_name}_{instance__#} )\n",
    "    * Normal the sizes of the individual cutouts (max_size= 480, on the longest side )\n",
    "* Thumbnails of the binary masks: (include, label in filename: mask_{resource_id}_{object_name}_{instance__#} )\n",
    "* Output the bounding box information -- JSON output (include, label in filename: image_{resource_id}_{object_name}_{instance__#}.JSON )\n",
    "    * Including the four coordinates (Normalized 0-1)\n",
    "\n",
    "Note: the outputs will be directed into the UI folders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General utility libraries\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Importing Pytorch ML Libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Importing the Models and their respective weights\n",
    "from torchvision.models.detection import (\n",
    "    # Faster R-CNN\n",
    "    fasterrcnn_resnet50_fpn_v2,\n",
    "    FasterRCNN_ResNet50_FPN_V2_Weights,\n",
    ")\n",
    "\n",
    "# Utility functions that help visualize the models and describe the model outputs.\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "from IPython.display import display\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# Libraries Mask manipulation and generation\n",
    "import cv2\n",
    "from scipy.ndimage import binary_dilation, binary_erosion, binary_closing\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "from workflow_helpers import *\n",
    "\n",
    "# from utilities import show_anns_ours, run_ours_box_or_points\n",
    "# ** For now, I have opted to not include these imports. I will define the \n",
    "# functions manually and later add to a `helper/utility.py` script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II. Creation of a Model Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many Items do you want to output? Refer to the Notebook 1 value to output the same amount.\n",
    "\n",
    "number_of_instances = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_2023632670.jpg 2023632670\n",
      "image_89709659.jpg 89709659\n",
      "image_2011631485.jpg 2011631485\n",
      "image_2020742358.jpg 2020742358\n",
      "image_2010651699.jpg 2010651699\n",
      "image_2006681388.jpg 2006681388\n",
      "image_2002705621.jpg 2002705621\n",
      "image_97518968.jpg 97518968\n",
      "image_2016826637.jpg 2016826637\n",
      "image_00652544.jpg 00652544\n"
     ]
    }
   ],
   "source": [
    "model_dictionary = {}\n",
    "model_dictionary['items'] = []\n",
    "\n",
    "for picture in os.listdir('image-collection-output/')[:number_of_instances]:\n",
    "    item_dictionary = {}\n",
    "    resource_id = extract_number(picture)\n",
    "    item_dictionary['resource_id'] = resource_id\n",
    "    model_dictionary['items'].append(item_dictionary)\n",
    "    print(picture,resource_id)\n",
    "\n",
    "# model_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III. Create Resource Thumbnail.\n",
    "* Creates resource thumbnail using main dictionary.\n",
    "* Sends Thumbnail to UI.\n",
    "* Adds Thumbnail path to dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_main_thumbnail(image_path, output_path, item):\n",
    "    # thumbnail_name\n",
    "    # resource = os.path.basename(image_path)\n",
    "    base_name = os.path.basename(image_path).split('.')[0]\n",
    "\n",
    "    # Create Resource Thumbname\n",
    "    thumbnail_image = Image.open(image_path)\n",
    "    original_size = thumbnail_image.size\n",
    "    max_size = (480,480)\n",
    "    thumbnail_image.thumbnail(max_size)\n",
    "\n",
    "    # Create Output directory if it doesn't exist\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    thumbnail_name = f'{base_name}_thumbnail' + '.jpg'\n",
    "    output_filename =  os.path.join(output_path,thumbnail_name)  \n",
    "    thumbnail_image.save(output_filename)\n",
    "    print(f'Saved {thumbnail_name}')\n",
    "\n",
    "    item['original_format'] = original_size\n",
    "    item['thumbnail'] = thumbnail_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'resource_id': '2023632670'}\n",
      "{'resource_id': '89709659'}\n",
      "{'resource_id': '2011631485'}\n",
      "{'resource_id': '2020742358'}\n",
      "{'resource_id': '2010651699'}\n",
      "{'resource_id': '2006681388'}\n",
      "{'resource_id': '2002705621'}\n",
      "{'resource_id': '97518968'}\n",
      "{'resource_id': '2016826637'}\n",
      "{'resource_id': '00652544'}\n"
     ]
    }
   ],
   "source": [
    "for item in model_dictionary['items']:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image_2023632670_thumbnail.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image_89709659_thumbnail.jpg\n",
      "Saved image_2011631485_thumbnail.jpg\n",
      "Saved image_2020742358_thumbnail.jpg\n",
      "Saved image_2010651699_thumbnail.jpg\n",
      "Saved image_2006681388_thumbnail.jpg\n",
      "Saved image_2002705621_thumbnail.jpg\n",
      "Saved image_97518968_thumbnail.jpg\n",
      "Saved image_2016826637_thumbnail.jpg\n",
      "Saved image_00652544_thumbnail.jpg\n"
     ]
    }
   ],
   "source": [
    "for item in model_dictionary['items']:\n",
    "    id = item['resource_id']\n",
    "    # workflow/image-collection-output/image_2016631670.jpg\n",
    "\n",
    "    image = f'../workflow/image-collection-output/image_{id}.jpg'\n",
    "    create_main_thumbnail(image,'../ui/dummy-data', item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IV. Load the Faster-RCNN Model and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\n",
    "model.eval()\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V. Load the EfficientSAM Model from it's directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the EfficientSAM Model and setting the correct directoy\n",
    "parent_dir = os.getcwd()\n",
    "os.chdir(\"../early_work/efficient_sam/EfficientSAM\")\n",
    "\n",
    "from efficient_sam.build_efficient_sam import build_efficient_sam_vitt, build_efficient_sam_vits\n",
    "import zipfile\n",
    "\n",
    "efficient_sam_vitt_model = build_efficient_sam_vitt()\n",
    "efficient_sam_vitt_model.eval()\n",
    "\n",
    "# Since EfficientSAM-S checkpoint file is >100MB, we store the zip file.\n",
    "with zipfile.ZipFile(\"weights/efficient_sam_vits.pt.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"weights\")\n",
    "efficient_sam_vits_model = build_efficient_sam_vits()\n",
    "efficient_sam_vits_model.eval()\n",
    "\n",
    "os.chdir(parent_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_path, output_path, item, structuring_value=25,threshold =0.9):\n",
    "    # Read the image\n",
    "    img = read_image(image_path)\n",
    "\n",
    "    batch = [preprocess(img)]\n",
    "    # Get prediction from the model\n",
    "    prediction = model(batch)[0]\n",
    "    \n",
    "    if len(prediction['labels']) == 0:\n",
    "        print(f'No Object Detection predictions within the Scope of MS COCO dataset: {os.path.basename(image_path)}')\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Extracting the len of Index of the scores that meet the threshold value:\n",
    "        score_len = (prediction[\"scores\"] >= threshold).sum().item()\n",
    "        # Limits the scores at the threshold to just the top 5\n",
    "        if score_len >= 3:\n",
    "            score_len = 3\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "        resource = os.path.basename(image_path)\n",
    "        base_name = os.path.basename(image_path).split('.')[0]\n",
    "        resource_id = item['resource_id']\n",
    "        item['segments'] = []\n",
    "\n",
    "        for i in range(score_len):                \n",
    "            segment = {}\n",
    "            segment['bounding_box'] = prediction['boxes'].tolist()[i]\n",
    "            # Extract bounding box coordinates\n",
    "\n",
    "            bbox = segment['bounding_box']\n",
    "            x1 = bbox[0]\n",
    "            y1 = bbox[1]\n",
    "            x2 = bbox[2]\n",
    "            y2 = bbox[3]\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "\n",
    "            class_index = prediction['labels'][i].item()\n",
    "            class_label = weights.meta[\"categories\"][class_index]\n",
    "            # print(class_label)\n",
    "\n",
    "            \n",
    "            # fig, ax = plt.subplots(1, 3, figsize=(30, 30))\n",
    "            input_point = np.array([[x1, y1], [x2, y2]])\n",
    "            input_label = np.array([2, 3])\n",
    "            \n",
    "\n",
    "            mask_efficient_sam_vitt = run_ours_box_or_points(image_path, input_point, input_label, efficient_sam_vitt_model)\n",
    "            # show_anns_ours(mask_efficient_sam_vitt, ax[1])\n",
    "            binary_mask = mask_efficient_sam_vitt\n",
    "            structuring_element = np.ones((structuring_value,structuring_value), dtype=bool)\n",
    "            dilated_mask = binary_dilation(binary_mask, structure=structuring_element)\n",
    "            eroded_mask = binary_erosion(dilated_mask, structure=structuring_element)\n",
    "\n",
    "            closed_mask_uint8 = (eroded_mask * 255).astype(np.uint8)\n",
    "\n",
    "            mask_name = f'mask_{resource_id}_{class_label}_{i}' + '.png'\n",
    "            mask_path = os.path.join(output_path, f'masks/{mask_name}')\n",
    "            cv2.imwrite(mask_path, closed_mask_uint8)\n",
    "            img_val = cv2.imread(image_path) \n",
    "            mask = cv2.imread(mask_path)\n",
    "\n",
    "            img_foreground = np.array((mask/255)*(img_val/255)) * img_val\n",
    "            na = img_foreground\n",
    "            \n",
    "\n",
    "            '''\n",
    "            Import to note that part of the following code is from substack\n",
    "            '''\n",
    "            # Make a True/False mask of pixels whose BGR values sum to more than zero\n",
    "            alpha = np.sum(na, axis=-1) > 0\n",
    "\n",
    "            # Convert True/False to 0/255 and change type to \"uint8\" to match \"na\"\n",
    "            alpha = np.uint8(alpha * 255)\n",
    "\n",
    "            # Stack new alpha layer with existing image to go from BGR to BGRA, i.e. 3 channels to 4 channels\n",
    "            res = np.dstack((na, alpha))\n",
    "            img = Image.fromarray(res, mode='RGBa')\n",
    "\n",
    "            # Save result\n",
    "            cutout_name =  f'cutout_{resource_id}_{class_label}_{i}' + '.png'\n",
    "            cutout_path = os.path.join(output_path, f'cutouts/{cutout_name}')\n",
    "            cv2.imwrite(cutout_path, res)\n",
    "            \n",
    "            resize_to_thumbnail(cutout_path)\n",
    "            resize_to_thumbnail(mask_path)\n",
    "\n",
    "            segment['confidence'] = prediction[\"scores\"][i].item()\n",
    "            segment['label'] =  class_label\n",
    "            segment['cutout'] = cutout_name\n",
    "            segment['mask'] =  mask_name\n",
    "            item['segments'].append(segment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Object Detection predictions within the Scope of MS COCO dataset: image_2011631485.jpg\n"
     ]
    }
   ],
   "source": [
    "for item in model_dictionary['items']:\n",
    "    id = item['resource_id']\n",
    "    image = f'../workflow/image-collection-output/image_{id}.jpg'\n",
    "    process_image(image,'../ui/dummy-data', item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"model_results.json\", 'w') as f:\n",
    "        json.dump(model_dictionary, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LOC)",
   "language": "python",
   "name": "loc-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
