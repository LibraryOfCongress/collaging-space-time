{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Object Detection and Image Segmentation \n",
    "\n",
    "Using the images downloaded from the Library of Congress API in [Step 1 (Metadata Collection and Image Download)]('https://github.com/beefoo/lclabs-jfp24/blob/main/workflow/step_1_metadata_and_image_download.ipynb), the second step in our workflow will focus on the computer vision aspects of the collage tool. \n",
    "\n",
    "This step utilizes PyTorch's Faster R-CNN object detection model and weights to generate information regarding predicted objects in an image (prediction confidence, class label, and bounding box). Subsequently, the bounding box information is used to supply a box-prompt to the segmentation model, EfficientSAM, which generates a mask (outline) of the object for extraction. In addition, this notebook also generates thumbnails to connect to the website's UI.\n",
    "\n",
    "Overall, outputs from both of these computer vision models are used to generate masks, extract objects from images, and generate data that is stored as a JSON file ('model_results.json')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Imports\n",
    "\n",
    "Importing all necessary libraries and modules. During your first run, it may take some time to import the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General utility libraries\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Importing Pytorch ML Libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Importing the Models and their respective weights\n",
    "from torchvision.models.detection import (\n",
    "    # Faster R-CNN\n",
    "    fasterrcnn_resnet50_fpn_v2,\n",
    "    FasterRCNN_ResNet50_FPN_V2_Weights,\n",
    ")\n",
    "\n",
    "# Utility functions that help visualize the models and describe the model outputs.\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "from IPython.display import display\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# Libraries Mask manipulation and generation\n",
    "import cv2\n",
    "from scipy.ndimage import binary_dilation, binary_erosion, binary_closing\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "from workflow_helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Create Directories and Model Results Dictionary\n",
    "\n",
    "Outputs from the from the computer vision models will be stored as JSON. This part focuses on the creation of the directories that will store the data and the dictionaries which will eventually be turned into the final JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many Items do you want to output? Refer to the Notebook 1 value to output the same amount.\n",
    "number_of_instances = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Directories for reference\n",
    "root_directory = os.getcwd()\n",
    "data_directory = \"workflow_data\"\n",
    "output_directory = os.path.join(data_directory, \"image-collection-output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dictionary = {}\n",
    "model_dictionary['items'] = []\n",
    "\n",
    "for picture in os.listdir('image-collection-output/')[:number_of_instances]:\n",
    "    if picture != '.DS_Store':\n",
    "        item_dictionary = {}\n",
    "        resource_id = extract_number(picture)\n",
    "        item_dictionary['resource_id'] = resource_id\n",
    "        model_dictionary['items'].append(item_dictionary)\n",
    "        print(picture,resource_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Create Item Thumbnail\n",
    "\n",
    "Image thumbnails are created using the 'items_metadata.json' generated in Step 1. The thumbnails are output to the tool's UI, and the file paths are stored in the model_results dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_main_thumbnail(image_path, output_path, item):\n",
    "    # thumbnail_name\n",
    "    # resource = os.path.basename(image_path)\n",
    "    base_name = os.path.basename(image_path).split('.')[0]\n",
    "\n",
    "    # Create Resource Thumbname\n",
    "    thumbnail_image = Image.open(image_path)\n",
    "    original_size = thumbnail_image.size\n",
    "    max_size = (480,480)\n",
    "    thumbnail_image.thumbnail(max_size)\n",
    "\n",
    "    # Create Output directory if it doesn't exist\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    thumbnail_name = f'{base_name}_thumbnail' + '.jpg'\n",
    "    output_filename =  os.path.join(output_path,thumbnail_name)  \n",
    "    thumbnail_image.save(output_filename)\n",
    "    print(f'Saved {thumbnail_name}')\n",
    "\n",
    "    # Saves original format and thumbnail filename to dictionary.\n",
    "    item['original_format'] = original_size\n",
    "    item['thumbnail'] = thumbnail_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in model_dictionary['items']:\n",
    "    id = item['resource_id']\n",
    "\n",
    "    image = f'../workflow/image-collection-output/image_{id}.jpg'\n",
    "    create_main_thumbnail(image,'../ui/dummy-data', item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Load the Faster-RCNN Model and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "\n",
    "# Loading the \n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\n",
    "model.eval()\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. Load the EfficientSAM Model\n",
    "\n",
    "Unlike the PyTorch model, the EfficientSAM model and weights must be downloaded locally. The cell block below pulls the original repository to generate the model. Expect it to take some time on the first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.getcwd()\n",
    "\n",
    "path_dit = os.path.join(parent_dir,'EfficientSAM')\n",
    "\n",
    "if not os.path.exists(path_dit):\n",
    "    !git clone https://github.com/yformer/EfficientSAM.git\n",
    "    \n",
    "os.chdir(\"EfficientSAM\")\n",
    "\n",
    "# Importing the EfficientSAM Model and setting the correct directoy\n",
    "from efficient_sam.build_efficient_sam import build_efficient_sam_vitt, build_efficient_sam_vits\n",
    "import zipfile\n",
    "\n",
    "efficient_sam_vitt_model = build_efficient_sam_vitt()\n",
    "efficient_sam_vitt_model.eval()\n",
    "\n",
    "# Since EfficientSAM-S checkpoint file is >100MB, we store the zip file.\n",
    "with zipfile.ZipFile(\"weights/efficient_sam_vits.pt.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"weights\")\n",
    "efficient_sam_vits_model = build_efficient_sam_vits()\n",
    "efficient_sam_vits_model.eval()\n",
    "\n",
    "os.chdir(parent_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_path, output_path, item, structuring_value=25,threshold =0.9):\n",
    "    # Read the image\n",
    "    img = read_image(image_path)\n",
    "\n",
    "    batch = [preprocess(img)]\n",
    "    # Get prediction from the model\n",
    "    prediction = model(batch)[0]\n",
    "    \n",
    "    if len(prediction['labels']) == 0:\n",
    "        print(f'No Object Detection predictions within the Scope of MS COCO dataset: {os.path.basename(image_path)}')\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Extracting the len of Index of the scores that meet the threshold value:\n",
    "        score_len = (prediction[\"scores\"] >= threshold).sum().item()\n",
    "        # Limits the scores at the threshold to just the top 5\n",
    "        if score_len >= 3:\n",
    "            score_len = 3\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "        resource = os.path.basename(image_path)\n",
    "        base_name = os.path.basename(image_path).split('.')[0]\n",
    "        resource_id = item['resource_id']\n",
    "        item['segments'] = []\n",
    "\n",
    "        for i in range(score_len):                \n",
    "            segment = {}\n",
    "            bbox =  prediction['boxes'].tolist()[i]\n",
    "            # Extract bounding box coordinates\n",
    "\n",
    "            x1 = bbox[0]\n",
    "            y1 = bbox[1]\n",
    "            x2 = bbox[2]\n",
    "            y2 = bbox[3]\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "\n",
    "            if (h*w) <= 30000: \n",
    "                continue\n",
    "            else:\n",
    "                class_index = prediction['labels'][i].item()\n",
    "                class_label = weights.meta[\"categories\"][class_index]\n",
    "                # print(class_label)\n",
    "\n",
    "                \n",
    "                # fig, ax = plt.subplots(1, 3, figsize=(30, 30))\n",
    "                input_point = np.array([[x1, y1], [x2, y2]])\n",
    "                input_label = np.array([2, 3])\n",
    "                \n",
    "\n",
    "                mask_efficient_sam_vitt = run_ours_box_or_points(image_path, input_point, input_label, efficient_sam_vitt_model)\n",
    "                # show_anns_ours(mask_efficient_sam_vitt, ax[1])\n",
    "                binary_mask = mask_efficient_sam_vitt\n",
    "                structuring_element = np.ones((structuring_value,structuring_value), dtype=bool)\n",
    "                dilated_mask = binary_dilation(binary_mask, structure=structuring_element)\n",
    "                eroded_mask = binary_erosion(dilated_mask, structure=structuring_element)\n",
    "\n",
    "                closed_mask_uint8 = (eroded_mask * 255).astype(np.uint8)\n",
    "\n",
    "                mask_name = f'mask_{resource_id}_{class_label}_{i}' + '.png'\n",
    "                mask_path = os.path.join(output_path, f'masks/{mask_name}')\n",
    "                cv2.imwrite(mask_path, closed_mask_uint8)\n",
    "                img_val = cv2.imread(image_path) \n",
    "                mask = cv2.imread(mask_path)\n",
    "\n",
    "                img_foreground = np.array((mask/255)*(img_val/255)) * img_val\n",
    "                na = img_foreground\n",
    "                \n",
    "\n",
    "                '''\n",
    "                Import to note that part of the following code is from substack\n",
    "                '''\n",
    "                # Make a True/False mask of pixels whose BGR values sum to more than zero\n",
    "                alpha = np.sum(na, axis=-1) > 0\n",
    "\n",
    "                # Convert True/False to 0/255 and change type to \"uint8\" to match \"na\"\n",
    "                alpha = np.uint8(alpha * 255)\n",
    "\n",
    "                # Stack new alpha layer with existing image to go from BGR to BGRA, i.e. 3 channels to 4 channels\n",
    "                res = np.dstack((na, alpha))\n",
    "                img = Image.fromarray(res, mode='RGBa')\n",
    "\n",
    "                # Save result\n",
    "                cutout_name =  f'cutout_{resource_id}_{class_label}_{i}' + '.png'\n",
    "                cutout_path = os.path.join(output_path, f'cutouts/{cutout_name}')\n",
    "                cv2.imwrite(cutout_path, res)\n",
    "                \n",
    "                crop_image(cutout_path, x1, y1, x2, y2)\n",
    "\n",
    "                resize_to_thumbnail(cutout_path)\n",
    "                resize_to_thumbnail(mask_path)\n",
    "\n",
    "                segment['confidence'] = prediction[\"scores\"][i].item()\n",
    "                segment['label'] =  class_label\n",
    "                segment['cutout'] = cutout_name\n",
    "                segment['mask'] =  mask_name\n",
    "                item['segments'].append(segment)\n",
    "                segment['bounding_box'] = bbox\n",
    "                segment['instance'] =  i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(root_directory)\n",
    "\n",
    "for item in model_dictionary['items']:\n",
    "   id = item['resource_id']\n",
    "   image = f'image-collection-output/image_{id}.jpg'\n",
    "   process_image(image,'../ui/dummy-data', item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_directory,f\"model_results.json\"), 'w') as f:\n",
    "        json.dump(model_dictionary, f, indent=4)\n",
    "\n",
    "print('All done! Model outputs stored as JSON and masks/extractions saved to UI')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
