{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combination of the object detection and segmentation. Would output the masks and the 'cutouts'\n",
    "\n",
    "* Original image as a thumbnail -- (480 on the longest side as jpegs) **[DONE]**\n",
    "* Output the individual cutouts as png (include, label in filename: image_{resource_id}_{object_name}_{instance__#} )\n",
    "    * Normal the sizes of the individual cutouts (max_size= 480, on the longest side )\n",
    "* Thumbnails of the binary masks: (include, label in filename: mask_{resource_id}_{object_name}_{instance__#} )\n",
    "* Output the bounding box information -- JSON output (include, label in filename: image_{resource_id}_{object_name}_{instance__#}.JSON )\n",
    "    * Including the four coordinates (Normalized 0-1)\n",
    "\n",
    "Note: the outputs will be directed into the UI folders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General utility libraries\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Importing Pytorch ML Libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Importing the Models and their respective weights\n",
    "from torchvision.models.detection import (\n",
    "    # Faster R-CNN\n",
    "    fasterrcnn_resnet50_fpn_v2,\n",
    "    FasterRCNN_ResNet50_FPN_V2_Weights,\n",
    ")\n",
    "\n",
    "# Utility functions that help visualize the models and describe the model outputs.\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from PIL import ImageFont, ImageDraw\n",
    "from IPython.display import display\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# Libraries Mask manipulation and generation\n",
    "import cv2\n",
    "from scipy.ndimage import binary_dilation, binary_erosion, binary_closing\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "\n",
    "# from utilities import show_anns_ours, run_ours_box_or_points\n",
    "# ** For now, I have opted to not include these imports. I will define the \n",
    "# functions manually and later add to a `helper/utility.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_anns_ours(mask, ax):\n",
    "    ax.set_autoscale_on(False)\n",
    "    img = np.ones((mask.shape[0], mask.shape[1], 4))\n",
    "    img[:, :, 3] = 0\n",
    "    color_mask = [0, 1, 0, 0.7]\n",
    "    img[np.logical_not(mask)] = color_mask\n",
    "    ax.imshow(img)\n",
    "\n",
    "\n",
    "def run_ours_box_or_points(img_path, pts_sampled, pts_labels, model):\n",
    "    image_np = np.array(Image.open(img_path))\n",
    "    img_tensor = ToTensor()(image_np)\n",
    "    pts_sampled = torch.reshape(torch.tensor(pts_sampled), [1, 1, -1, 2])\n",
    "    pts_labels = torch.reshape(torch.tensor(pts_labels), [1, 1, -1])\n",
    "    predicted_logits, predicted_iou = model(\n",
    "        img_tensor[None, ...],\n",
    "        pts_sampled,\n",
    "        pts_labels,\n",
    "    )\n",
    "\n",
    "    sorted_ids = torch.argsort(predicted_iou, dim=-1, descending=True)\n",
    "    predicted_iou = torch.take_along_dim(predicted_iou, sorted_ids, dim=2)\n",
    "    predicted_logits = torch.take_along_dim(\n",
    "        predicted_logits, sorted_ids[..., None, None], dim=2\n",
    "    )\n",
    "\n",
    "    return torch.ge(predicted_logits[0, 0, 0, :, :], 0).cpu().detach().numpy()\n",
    "\n",
    "def resize_to_thumbnail(path):\n",
    "    img = Image.open(path)\n",
    "    img.thumbnail((480,480))\n",
    "    img.save(path)\n",
    "\n",
    "def crop_image(path,x1,y1,x2,y2):\n",
    "    img = Image.open(path)\n",
    "    box = (x1-20, y1-20, x2+20, y2+30)\n",
    "    img = img.crop(box)\n",
    "    img.save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II. Creation of a Dummy Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number(directory_name):\n",
    "    match = re.findall(r\"\\d+\", directory_name)\n",
    "    # if match:\n",
    "    return int(match[0])\n",
    "\n",
    "dummy_dictionary = {}\n",
    "for picture in os.listdir('../early_work/images')[:1]:\n",
    "    resource_info = {}\n",
    "    resource_id = extract_number(picture)\n",
    "    resource_info['resource_id'] = resource_id\n",
    "    dummy_dictionary[picture] = resource_info\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III. Create Resource Thumbnail.\n",
    "* Creates resource thumbnail using main dictionary.\n",
    "* Sends Thumbnail to UI.\n",
    "* Adds Thumbnail path to dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_main_thumbnail(image_path, output_path, resource_dictionary):\n",
    "    # thumbnail_name\n",
    "    resource = os.path.basename(image_path)\n",
    "    base_name = os.path.basename(image_path).split('.')[0]\n",
    "\n",
    "    # Create Resource Thumbname\n",
    "    thumbnail_image = Image.open(image_path)\n",
    "    original_size = thumbnail_image.size\n",
    "    max_size = (480,480)\n",
    "    thumbnail_image.thumbnail(max_size)\n",
    "\n",
    "    # Create Output directory if it doesn't exist\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    thumbnail_name = f'{base_name}_thumbnail' + '.jpg'\n",
    "    output_filename =  os.path.join(output_path,thumbnail_name)  \n",
    "    thumbnail_image.save(output_filename)\n",
    "    print(f'Saved {thumbnail_name}')\n",
    "\n",
    "    resource_dictionary[resource]['original_format'] = original_size\n",
    "    resource_dictionary[resource]['thumbnail'] = thumbnail_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image_2017454465_thumbnail.jpg\n",
      "Saved image_2007684280_thumbnail.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image_2011632159_thumbnail.jpg\n",
      "Saved image_2017878880_thumbnail.jpg\n",
      "Saved image_2003654393_thumbnail.jpg\n",
      "Saved image_88694120_thumbnail.jpg\n",
      "Saved image_00650949_thumbnail.jpg\n",
      "Saved image_2016869441_thumbnail.jpg\n",
      "Saved image_2010641712_thumbnail.jpg\n",
      "Saved image_2016866957_thumbnail.jpg\n",
      "Saved image_2016873397_thumbnail.jpg\n",
      "Saved image_00650962_thumbnail.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'image_2017454465.jpg': {'resource_id': 2017454465,\n",
       "  'original_format': (4019, 6252),\n",
       "  'thumbnail': 'image_2017454465_thumbnail.jpg'},\n",
       " 'image_2007684280.jpg': {'resource_id': 2007684280,\n",
       "  'original_format': (2604, 2151),\n",
       "  'thumbnail': 'image_2007684280_thumbnail.jpg'},\n",
       " 'image_2011632159.jpg': {'resource_id': 2011632159,\n",
       "  'original_format': (1213, 1690),\n",
       "  'thumbnail': 'image_2011632159_thumbnail.jpg'},\n",
       " 'image_2017878880.jpg': {'resource_id': 2017878880,\n",
       "  'original_format': (808, 1024),\n",
       "  'thumbnail': 'image_2017878880_thumbnail.jpg'},\n",
       " 'image_2003654393.jpg': {'resource_id': 2003654393,\n",
       "  'original_format': (1024, 705),\n",
       "  'thumbnail': 'image_2003654393_thumbnail.jpg'},\n",
       " 'image_88694120.jpg': {'resource_id': 88694120,\n",
       "  'original_format': (1205, 994),\n",
       "  'thumbnail': 'image_88694120_thumbnail.jpg'},\n",
       " 'image_00650949.jpg': {'resource_id': 650949,\n",
       "  'original_format': (1024, 628),\n",
       "  'thumbnail': 'image_00650949_thumbnail.jpg'},\n",
       " 'image_2016869441.jpg': {'resource_id': 2016869441,\n",
       "  'original_format': (741, 1024),\n",
       "  'thumbnail': 'image_2016869441_thumbnail.jpg'},\n",
       " 'image_2010641712.jpg': {'resource_id': 2010641712,\n",
       "  'original_format': (1794, 2529),\n",
       "  'thumbnail': 'image_2010641712_thumbnail.jpg'},\n",
       " 'image_2016866957.jpg': {'resource_id': 2016866957,\n",
       "  'original_format': (1024, 762),\n",
       "  'thumbnail': 'image_2016866957_thumbnail.jpg'},\n",
       " 'image_2016873397.jpg': {'resource_id': 2016873397,\n",
       "  'original_format': (1024, 825),\n",
       "  'thumbnail': 'image_2016873397_thumbnail.jpg'},\n",
       " 'image_00650962.jpg': {'resource_id': 650962,\n",
       "  'original_format': (1024, 803),\n",
       "  'thumbnail': 'image_00650962_thumbnail.jpg'}}"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for image in dummy_dictionary:\n",
    "    # print(image)\n",
    "    image = '../early_work/images/' + image\n",
    "    create_main_thumbnail(image,'../workflow_ui_sample/', dummy_dictionary)\n",
    "\n",
    "dummy_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IV. Load the Faster-RCNN Model and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\n",
    "model.eval()\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V. Load the EfficientSAM Model from it's directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the EfficientSAM Model and setting the correct directoy\n",
    "parent_dir = os.getcwd()\n",
    "os.chdir(\"../early_work/efficient_sam/EfficientSAM\")\n",
    "\n",
    "from efficient_sam.build_efficient_sam import build_efficient_sam_vitt, build_efficient_sam_vits\n",
    "import zipfile\n",
    "\n",
    "efficient_sam_vitt_model = build_efficient_sam_vitt()\n",
    "efficient_sam_vitt_model.eval()\n",
    "\n",
    "# Since EfficientSAM-S checkpoint file is >100MB, we store the zip file.\n",
    "with zipfile.ZipFile(\"weights/efficient_sam_vits.pt.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"weights\")\n",
    "efficient_sam_vits_model = build_efficient_sam_vits()\n",
    "efficient_sam_vits_model.eval()\n",
    "\n",
    "os.chdir(parent_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_path, output_path, resource_dictionary, structuring_value=25,threshold =0.9):\n",
    "    # Read the image\n",
    "    img = read_image(image_path)\n",
    "\n",
    "    batch = [preprocess(img)]\n",
    "    # Get prediction from the model\n",
    "    prediction = model(batch)[0]\n",
    "    \n",
    "    if len(prediction['labels']) == 0:\n",
    "        print(f'No Object Detection predictions within the Scope of MS COCO dataset: {os.path.basename(image_path)}')\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Extracting the len of Index of the scores that meet the threshold value:\n",
    "        score_len = (prediction[\"scores\"] >= threshold).sum().item()\n",
    "        # Limits the scores at the threshold to just the top 5\n",
    "        if score_len >= 3:\n",
    "            score_len = 3\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "        resource = os.path.basename(image_path)\n",
    "        base_name = os.path.basename(image_path).split('.')[0]\n",
    "        resource_id = resource_dictionary[resource]['resource_id']\n",
    "        resource_dictionary[resource]['segments'] = []\n",
    "\n",
    "        for i in range(score_len):                \n",
    "            segment = {}\n",
    "            segment['bounding_box'] = prediction['boxes'].tolist()[i]\n",
    "            # Extract bounding box coordinates\n",
    "\n",
    "            bbox = segment['bounding_box']\n",
    "            x1 = bbox[0]\n",
    "            y1 = bbox[1]\n",
    "            x2 = bbox[2]\n",
    "            y2 = bbox[3]\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "\n",
    "            class_index = prediction['labels'][i].item()\n",
    "            class_label = weights.meta[\"categories\"][class_index]\n",
    "            # print(class_label)\n",
    "\n",
    "            \n",
    "            # fig, ax = plt.subplots(1, 3, figsize=(30, 30))\n",
    "            input_point = np.array([[x1, y1], [x2, y2]])\n",
    "            input_label = np.array([2, 3])\n",
    "            \n",
    "\n",
    "            mask_efficient_sam_vitt = run_ours_box_or_points(image_path, input_point, input_label, efficient_sam_vitt_model)\n",
    "            # show_anns_ours(mask_efficient_sam_vitt, ax[1])\n",
    "            binary_mask = mask_efficient_sam_vitt\n",
    "            structuring_element = np.ones((structuring_value,structuring_value), dtype=bool)\n",
    "            dilated_mask = binary_dilation(binary_mask, structure=structuring_element)\n",
    "            eroded_mask = binary_erosion(dilated_mask, structure=structuring_element)\n",
    "\n",
    "            closed_mask_uint8 = (eroded_mask * 255).astype(np.uint8)\n",
    "\n",
    "            \n",
    "            mask_path = os.path.join(output_path, f'masks/mask_{resource_id}_{class_label}_{i}' + '.png')\n",
    "            cv2.imwrite(mask_path, closed_mask_uint8)\n",
    "            img_val = cv2.imread(image_path) \n",
    "            mask = cv2.imread(mask_path)\n",
    "\n",
    "            img_foreground = np.array((mask/255)*(img_val/255)) * img_val\n",
    "            na = img_foreground\n",
    "            \n",
    "\n",
    "            '''\n",
    "            Import to note that part of the following code is from substack\n",
    "            '''\n",
    "            # Make a True/False mask of pixels whose BGR values sum to more than zero\n",
    "            alpha = np.sum(na, axis=-1) > 0\n",
    "\n",
    "            # Convert True/False to 0/255 and change type to \"uint8\" to match \"na\"\n",
    "            alpha = np.uint8(alpha * 255)\n",
    "\n",
    "            # Stack new alpha layer with existing image to go from BGR to BGRA, i.e. 3 channels to 4 channels\n",
    "            res = np.dstack((na, alpha))\n",
    "            img = Image.fromarray(res, mode='RGBa')\n",
    "\n",
    "            # Save result\n",
    "\n",
    "            cutout_path = os.path.join(output_path, f'cutouts/cutout_{resource_id}_{class_label}_{i}' + '.png')\n",
    "            cv2.imwrite(cutout_path, res)\n",
    "            \n",
    "            crop_image(cutout_path,x1,y1,x2,y2)\n",
    "            crop_image(mask_path,x1,y1,x2,y2)\n",
    "            resize_to_thumbnail(cutout_path)\n",
    "            resize_to_thumbnail(mask_path)\n",
    "\n",
    "            segment['label'] =  class_label\n",
    "            segment['cutout'] = cutout_path\n",
    "            segment['mask'] =  mask_path\n",
    "\n",
    "\n",
    "            resource_dictionary[resource]['segments'].append(segment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Object Detection predictions within the Scope of MS COCO dataset: image_2011632159.jpg\n",
      "No Object Detection predictions within the Scope of MS COCO dataset: image_88694120.jpg\n"
     ]
    }
   ],
   "source": [
    "for image in dummy_dictionary:\n",
    "    # print(image)\n",
    "    image = '../early_work/images/' + image\n",
    "    # process_image(image,'../workflow_ui_sample', dummy_dictionary)\n",
    "    process_image(image,'../workflow_ui_sample/', dummy_dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open(\"dummy_manifest.json\", \"w\") as outfile: \n",
    "    json.dump(dummy_dictionary, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LOC)",
   "language": "python",
   "name": "loc-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
