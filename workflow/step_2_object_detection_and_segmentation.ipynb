{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Object Detection and Image Segmentation \n",
    "\n",
    "Using the images downloaded from the Library of Congress API in [Step 1 (Metadata Collection and Image Download)]('https://github.com/beefoo/lclabs-jfp24/blob/main/workflow/step_1_metadata_and_image_download.ipynb), the second step in our workflow will focus on the computer vision aspects of the collage tool. \n",
    "\n",
    "This step utilizes PyTorch's Faster R-CNN object detection model and weights to generate information regarding predicted objects in an image (prediction confidence, class label, and bounding box). Subsequently, the bounding box information is used to supply a box-prompt to the segmentation model, EfficientSAM, which generates a mask (outline) of the object for extraction. In addition, this notebook also generates thumbnails to connect to the website's UI.\n",
    "\n",
    "Overall, outputs from both of these computer vision models are used to generate masks, extract objects from images, and generate data that is stored as a JSON file ('model_results.json')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Imports\n",
    "\n",
    "Importing all necessary libraries and modules. During your first run, it may take some time to import the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General utility libraries\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Importing Pytorch ML Libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Importing the Models and their respective weights\n",
    "from torchvision.models.detection import (\n",
    "    # Faster R-CNN\n",
    "    fasterrcnn_resnet50_fpn_v2,\n",
    "    FasterRCNN_ResNet50_FPN_V2_Weights,\n",
    ")\n",
    "\n",
    "# Utility functions that help visualize the models and describe the model outputs.\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "from IPython.display import display\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# Libraries Mask manipulation and generation\n",
    "import cv2\n",
    "from scipy.ndimage import binary_dilation, binary_erosion, binary_closing\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "from workflow_helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Create Directories and Model Results Dictionary\n",
    "\n",
    "Outputs from the from the computer vision models will be stored as JSON. This part focuses on the creation of the directories that will store the data and the dictionaries which will eventually be turned into the final JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many Items do you want to output? Refer to the Notebook 1 value to output the same amount.\n",
    "number_of_instances = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Directories for reference\n",
    "root_directory = os.getcwd()\n",
    "data_directory = \"workflow_data\"\n",
    "output_directory = os.path.join(data_directory, \"image-collection-output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_2001699137.jpg 2001699137\n",
      "image_2001702332.jpg 2001702332\n",
      "image_2001703618.jpg 2001703618\n",
      "image_2001703638.jpg 2001703638\n",
      "image_2002705861.jpg 2002705861\n",
      "image_2002716781.jpg 2002716781\n",
      "image_2003666591.jpg 2003666591\n",
      "image_2003680531.jpg 2003680531\n",
      "image_2010630036.jpg 2010630036\n",
      "image_2010630192.jpg 2010630192\n",
      "image_2010630446.jpg 2010630446\n",
      "image_2010630700.jpg 2010630700\n",
      "image_2010641712.jpg 2010641712\n",
      "image_2010641826.jpg 2010641826\n",
      "image_2010648441.jpg 2010648441\n",
      "image_2010719313.jpg 2010719313\n",
      "image_2011630135.jpg 2011630135\n",
      "image_2011630582.jpg 2011630582\n",
      "image_2011630694.jpg 2011630694\n",
      "image_2011630889.jpg 2011630889\n",
      "image_2011631396.jpg 2011631396\n",
      "image_2011631448.jpg 2011631448\n",
      "image_2011631890.jpg 2011631890\n",
      "image_2011632545.jpg 2011632545\n",
      "image_2011632658.jpg 2011632658\n",
      "image_2011633142.jpg 2011633142\n",
      "image_2011633149.jpg 2011633149\n",
      "image_2011633233.jpg 2011633233\n",
      "image_2011634248.jpg 2011634248\n",
      "image_2011635657.jpg 2011635657\n",
      "image_2013634071.jpg 2013634071\n",
      "image_2013634076.jpg 2013634076\n",
      "image_2015645165.jpg 2015645165\n",
      "image_2015645184.jpg 2015645184\n",
      "image_2015646966.jpg 2015646966\n",
      "image_2016800361.jpg 2016800361\n",
      "image_2016855814.jpg 2016855814\n",
      "image_2016865984.jpg 2016865984\n",
      "image_2016866957.jpg 2016866957\n",
      "image_2016869441.jpg 2016869441\n",
      "image_2016870156.jpg 2016870156\n",
      "image_2016871072.jpg 2016871072\n",
      "image_2016871444.jpg 2016871444\n",
      "image_2016871505.jpg 2016871505\n",
      "image_2016871873.jpg 2016871873\n",
      "image_2016871965.jpg 2016871965\n",
      "image_2016872565.jpg 2016872565\n",
      "image_2016873397.jpg 2016873397\n",
      "image_2016873653.jpg 2016873653\n",
      "image_2016874827.jpg 2016874827\n",
      "image_2016874951.jpg 2016874951\n",
      "image_2016875338.jpg 2016875338\n",
      "image_2016875639.jpg 2016875639\n",
      "image_2016876665.jpg 2016876665\n",
      "image_2016878424.jpg 2016878424\n",
      "image_2016879575.jpg 2016879575\n",
      "image_2016883798.jpg 2016883798\n",
      "image_2016885398.jpg 2016885398\n",
      "image_2016887160.jpg 2016887160\n",
      "image_2016891178.jpg 2016891178\n",
      "image_2016892186.jpg 2016892186\n",
      "image_2016894037.jpg 2016894037\n",
      "image_2017696790.jpg 2017696790\n",
      "image_2017696833.jpg 2017696833\n",
      "image_2017764422.jpg 2017764422\n",
      "image_2017764973.jpg 2017764973\n",
      "image_2017765147.jpg 2017765147\n",
      "image_2017765150.jpg 2017765150\n",
      "image_2017765480.jpg 2017765480\n",
      "image_2017822324.jpg 2017822324\n",
      "image_2017825632.jpg 2017825632\n",
      "image_2017840870.jpg 2017840870\n",
      "image_2017848679.jpg 2017848679\n",
      "image_2017863004.jpg 2017863004\n",
      "image_2017863013.jpg 2017863013\n",
      "image_2017863029.jpg 2017863029\n",
      "image_2017863034.jpg 2017863034\n",
      "image_2017863265.jpg 2017863265\n",
      "image_2017863648.jpg 2017863648\n",
      "image_2017863919.jpg 2017863919\n",
      "image_2017878551.jpg 2017878551\n",
      "image_2018648196.jpg 2018648196\n",
      "image_2019631085.jpg 2019631085\n",
      "image_2019646232.jpg 2019646232\n",
      "image_2019646236.jpg 2019646236\n",
      "image_2019672846.jpg 2019672846\n",
      "image_2019680628.jpg 2019680628\n",
      "image_2019681158.jpg 2019681158\n",
      "image_2021630920.jpg 2021630920\n",
      "image_2021638548.jpg 2021638548\n",
      "image_96516051.jpg 96516051\n"
     ]
    }
   ],
   "source": [
    "model_dictionary = {}\n",
    "model_dictionary['items'] = []\n",
    "\n",
    "for picture in os.listdir('image-collection-output/')[:number_of_instances]:\n",
    "    if picture != '.DS_Store':\n",
    "        item_dictionary = {}\n",
    "        resource_id = extract_number(picture)\n",
    "        item_dictionary['resource_id'] = resource_id\n",
    "        model_dictionary['items'].append(item_dictionary)\n",
    "        print(picture,resource_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Create Item Thumbnail\n",
    "\n",
    "Image thumbnails are created using the 'items_metadata.json' generated in Step 1. The thumbnails are output to the tool's UI, and the file paths are stored in the model_results dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_main_thumbnail(image_path, output_path, item):\n",
    "    # thumbnail_name\n",
    "    # resource = os.path.basename(image_path)\n",
    "    base_name = os.path.basename(image_path).split('.')[0]\n",
    "\n",
    "    # Create Resource Thumbname\n",
    "    thumbnail_image = Image.open(image_path)\n",
    "    original_size = thumbnail_image.size\n",
    "    max_size = (480,480)\n",
    "    thumbnail_image.thumbnail(max_size)\n",
    "\n",
    "    # Create Output directory if it doesn't exist\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    thumbnail_name = f'{base_name}_thumbnail' + '.jpg'\n",
    "    output_filename =  os.path.join(output_path,thumbnail_name)  \n",
    "    thumbnail_image.save(output_filename)\n",
    "    print(f'Saved {thumbnail_name}')\n",
    "\n",
    "    # Saves original format and thumbnail filename to dictionary.\n",
    "    item['original_format'] = original_size\n",
    "    item['thumbnail'] = thumbnail_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image_2001699137_thumbnail.jpg\n",
      "Saved image_2001702332_thumbnail.jpg\n",
      "Saved image_2001703618_thumbnail.jpg\n",
      "Saved image_2001703638_thumbnail.jpg\n",
      "Saved image_2002705861_thumbnail.jpg\n",
      "Saved image_2002716781_thumbnail.jpg\n",
      "Saved image_2003666591_thumbnail.jpg\n",
      "Saved image_2003680531_thumbnail.jpg\n",
      "Saved image_2010630036_thumbnail.jpg\n",
      "Saved image_2010630192_thumbnail.jpg\n",
      "Saved image_2010630446_thumbnail.jpg\n",
      "Saved image_2010630700_thumbnail.jpg\n",
      "Saved image_2010641712_thumbnail.jpg\n",
      "Saved image_2010641826_thumbnail.jpg\n",
      "Saved image_2010648441_thumbnail.jpg\n",
      "Saved image_2010719313_thumbnail.jpg\n",
      "Saved image_2011630135_thumbnail.jpg\n",
      "Saved image_2011630582_thumbnail.jpg\n",
      "Saved image_2011630694_thumbnail.jpg\n",
      "Saved image_2011630889_thumbnail.jpg\n",
      "Saved image_2011631396_thumbnail.jpg\n",
      "Saved image_2011631448_thumbnail.jpg\n",
      "Saved image_2011631890_thumbnail.jpg\n",
      "Saved image_2011632545_thumbnail.jpg\n",
      "Saved image_2011632658_thumbnail.jpg\n",
      "Saved image_2011633142_thumbnail.jpg\n",
      "Saved image_2011633149_thumbnail.jpg\n",
      "Saved image_2011633233_thumbnail.jpg\n",
      "Saved image_2011634248_thumbnail.jpg\n",
      "Saved image_2011635657_thumbnail.jpg\n",
      "Saved image_2013634071_thumbnail.jpg\n",
      "Saved image_2013634076_thumbnail.jpg\n",
      "Saved image_2015645165_thumbnail.jpg\n",
      "Saved image_2015645184_thumbnail.jpg\n",
      "Saved image_2015646966_thumbnail.jpg\n",
      "Saved image_2016800361_thumbnail.jpg\n",
      "Saved image_2016855814_thumbnail.jpg\n",
      "Saved image_2016865984_thumbnail.jpg\n",
      "Saved image_2016866957_thumbnail.jpg\n",
      "Saved image_2016869441_thumbnail.jpg\n",
      "Saved image_2016870156_thumbnail.jpg\n",
      "Saved image_2016871072_thumbnail.jpg\n",
      "Saved image_2016871444_thumbnail.jpg\n",
      "Saved image_2016871505_thumbnail.jpg\n",
      "Saved image_2016871873_thumbnail.jpg\n",
      "Saved image_2016871965_thumbnail.jpg\n",
      "Saved image_2016872565_thumbnail.jpg\n",
      "Saved image_2016873397_thumbnail.jpg\n",
      "Saved image_2016873653_thumbnail.jpg\n",
      "Saved image_2016874827_thumbnail.jpg\n",
      "Saved image_2016874951_thumbnail.jpg\n",
      "Saved image_2016875338_thumbnail.jpg\n",
      "Saved image_2016875639_thumbnail.jpg\n",
      "Saved image_2016876665_thumbnail.jpg\n",
      "Saved image_2016878424_thumbnail.jpg\n",
      "Saved image_2016879575_thumbnail.jpg\n",
      "Saved image_2016883798_thumbnail.jpg\n",
      "Saved image_2016885398_thumbnail.jpg\n",
      "Saved image_2016887160_thumbnail.jpg\n",
      "Saved image_2016891178_thumbnail.jpg\n",
      "Saved image_2016892186_thumbnail.jpg\n",
      "Saved image_2016894037_thumbnail.jpg\n",
      "Saved image_2017696790_thumbnail.jpg\n",
      "Saved image_2017696833_thumbnail.jpg\n",
      "Saved image_2017764422_thumbnail.jpg\n",
      "Saved image_2017764973_thumbnail.jpg\n",
      "Saved image_2017765147_thumbnail.jpg\n",
      "Saved image_2017765150_thumbnail.jpg\n",
      "Saved image_2017765480_thumbnail.jpg\n",
      "Saved image_2017822324_thumbnail.jpg\n",
      "Saved image_2017825632_thumbnail.jpg\n",
      "Saved image_2017840870_thumbnail.jpg\n",
      "Saved image_2017848679_thumbnail.jpg\n",
      "Saved image_2017863004_thumbnail.jpg\n",
      "Saved image_2017863013_thumbnail.jpg\n",
      "Saved image_2017863029_thumbnail.jpg\n",
      "Saved image_2017863034_thumbnail.jpg\n",
      "Saved image_2017863265_thumbnail.jpg\n",
      "Saved image_2017863648_thumbnail.jpg\n",
      "Saved image_2017863919_thumbnail.jpg\n",
      "Saved image_2017878551_thumbnail.jpg\n",
      "Saved image_2018648196_thumbnail.jpg\n",
      "Saved image_2019631085_thumbnail.jpg\n",
      "Saved image_2019646232_thumbnail.jpg\n",
      "Saved image_2019646236_thumbnail.jpg\n",
      "Saved image_2019672846_thumbnail.jpg\n",
      "Saved image_2019680628_thumbnail.jpg\n",
      "Saved image_2019681158_thumbnail.jpg\n",
      "Saved image_2021630920_thumbnail.jpg\n",
      "Saved image_2021638548_thumbnail.jpg\n",
      "Saved image_96516051_thumbnail.jpg\n"
     ]
    }
   ],
   "source": [
    "for item in model_dictionary['items']:\n",
    "    id = item['resource_id']\n",
    "\n",
    "    image = f'../workflow/image-collection-output/image_{id}.jpg'\n",
    "    create_main_thumbnail(image,'../ui/data', item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Load the Faster-RCNN Model and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "\n",
    "# Loading the \n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\n",
    "model.eval()\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. Load the EfficientSAM Model\n",
    "\n",
    "Unlike the PyTorch model, the EfficientSAM model and weights must be downloaded locally. The cell block below pulls the original repository to generate the model. Expect it to take some time on the first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'EfficientSAM'...\n",
      "Updating files:  92% (35/38)\n",
      "Updating files:  94% (36/38)\n",
      "Updating files:  97% (37/38)\n",
      "Updating files: 100% (38/38)\n",
      "Updating files: 100% (38/38), done.\n"
     ]
    }
   ],
   "source": [
    "parent_dir = os.getcwd()\n",
    "\n",
    "path_dit = os.path.join(parent_dir,'EfficientSAM')\n",
    "\n",
    "if not os.path.exists(path_dit):\n",
    "    !git clone https://github.com/yformer/EfficientSAM.git\n",
    "    \n",
    "os.chdir(\"EfficientSAM\")\n",
    "\n",
    "# Importing the EfficientSAM Model and setting the correct directoy\n",
    "from efficient_sam.build_efficient_sam import build_efficient_sam_vitt, build_efficient_sam_vits\n",
    "import zipfile\n",
    "\n",
    "efficient_sam_vitt_model = build_efficient_sam_vitt()\n",
    "efficient_sam_vitt_model.eval()\n",
    "\n",
    "# Since EfficientSAM-S checkpoint file is >100MB, we store the zip file.\n",
    "with zipfile.ZipFile(\"weights/efficient_sam_vits.pt.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"weights\")\n",
    "efficient_sam_vits_model = build_efficient_sam_vits()\n",
    "efficient_sam_vits_model.eval()\n",
    "\n",
    "os.chdir(parent_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_path, output_path, item, structuring_value=25,threshold =0.9):\n",
    "    # Read the image\n",
    "    img = read_image(image_path)\n",
    "\n",
    "    batch = [preprocess(img)]\n",
    "    # Get prediction from the model\n",
    "    prediction = model(batch)[0]\n",
    "    \n",
    "    if len(prediction['labels']) == 0:\n",
    "        print(f'No Object Detection predictions within the Scope of MS COCO dataset: {os.path.basename(image_path)}')\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Extracting the len of Index of the scores that meet the threshold value:\n",
    "        score_len = (prediction[\"scores\"] >= threshold).sum().item()\n",
    "        # Limits the scores at the threshold to just the top 5\n",
    "        if score_len >= 3:\n",
    "            score_len = 3\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "        resource = os.path.basename(image_path)\n",
    "        base_name = os.path.basename(image_path).split('.')[0]\n",
    "        resource_id = item['resource_id']\n",
    "        item['segments'] = []\n",
    "\n",
    "        for i in range(score_len):                \n",
    "            segment = {}\n",
    "            bbox =  prediction['boxes'].tolist()[i]\n",
    "            # Extract bounding box coordinates\n",
    "\n",
    "            x1 = bbox[0]\n",
    "            y1 = bbox[1]\n",
    "            x2 = bbox[2]\n",
    "            y2 = bbox[3]\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "\n",
    "            if (h*w) <= 30000: \n",
    "                continue\n",
    "            else:\n",
    "                class_index = prediction['labels'][i].item()\n",
    "                class_label = weights.meta[\"categories\"][class_index]\n",
    "                # print(class_label)\n",
    "\n",
    "                \n",
    "                # fig, ax = plt.subplots(1, 3, figsize=(30, 30))\n",
    "                input_point = np.array([[x1, y1], [x2, y2]])\n",
    "                input_label = np.array([2, 3])\n",
    "                \n",
    "\n",
    "                mask_efficient_sam_vitt = run_ours_box_or_points(image_path, input_point, input_label, efficient_sam_vitt_model)\n",
    "                # show_anns_ours(mask_efficient_sam_vitt, ax[1])\n",
    "                binary_mask = mask_efficient_sam_vitt\n",
    "                structuring_element = np.ones((structuring_value,structuring_value), dtype=bool)\n",
    "                dilated_mask = binary_dilation(binary_mask, structure=structuring_element)\n",
    "                eroded_mask = binary_erosion(dilated_mask, structure=structuring_element)\n",
    "\n",
    "                closed_mask_uint8 = (eroded_mask * 255).astype(np.uint8)\n",
    "\n",
    "                mask_name = f'mask_{resource_id}_{class_label}_{i}' + '.png'\n",
    "                mask_path = os.path.join(output_path, f'masks/{mask_name}')\n",
    "                cv2.imwrite(mask_path, closed_mask_uint8)\n",
    "                img_val = cv2.imread(image_path) \n",
    "                mask = cv2.imread(mask_path)\n",
    "\n",
    "                img_foreground = np.array((mask/255)*(img_val/255)) * img_val\n",
    "                na = img_foreground\n",
    "                \n",
    "\n",
    "                '''\n",
    "                Import to note that part of the following code is from substack\n",
    "                '''\n",
    "                # Make a True/False mask of pixels whose BGR values sum to more than zero\n",
    "                alpha = np.sum(na, axis=-1) > 0\n",
    "\n",
    "                # Convert True/False to 0/255 and change type to \"uint8\" to match \"na\"\n",
    "                alpha = np.uint8(alpha * 255)\n",
    "\n",
    "                # Stack new alpha layer with existing image to go from BGR to BGRA, i.e. 3 channels to 4 channels\n",
    "                res = np.dstack((na, alpha))\n",
    "                img = Image.fromarray(res, mode='RGBa')\n",
    "\n",
    "                # Save result\n",
    "                cutout_name =  f'cutout_{resource_id}_{class_label}_{i}' + '.png'\n",
    "                cutout_path = os.path.join(output_path, f'cutouts/{cutout_name}')\n",
    "                cv2.imwrite(cutout_path, res)\n",
    "                \n",
    "                crop_image(cutout_path, x1, y1, x2, y2)\n",
    "\n",
    "                resize_to_thumbnail(cutout_path)\n",
    "                resize_to_thumbnail(mask_path)\n",
    "\n",
    "                segment['confidence'] = prediction[\"scores\"][i].item()\n",
    "                segment['label'] =  class_label\n",
    "                segment['cutout'] = cutout_name\n",
    "                segment['mask'] =  mask_name\n",
    "                item['segments'].append(segment)\n",
    "                segment['bounding_box'] = bbox\n",
    "                segment['instance'] =  i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Object Detection predictions within the Scope of MS COCO dataset: image_2010630036.jpg\n",
      "No Object Detection predictions within the Scope of MS COCO dataset: image_2010630192.jpg\n",
      "No Object Detection predictions within the Scope of MS COCO dataset: image_2010630700.jpg\n",
      "No Object Detection predictions within the Scope of MS COCO dataset: image_2010641826.jpg\n",
      "No Object Detection predictions within the Scope of MS COCO dataset: image_2011630135.jpg\n",
      "No Object Detection predictions within the Scope of MS COCO dataset: image_2011630582.jpg\n",
      "No Object Detection predictions within the Scope of MS COCO dataset: image_2011630694.jpg\n",
      "No Object Detection predictions within the Scope of MS COCO dataset: image_2011630889.jpg\n"
     ]
    }
   ],
   "source": [
    "os.chdir(root_directory)\n",
    "\n",
    "for item in model_dictionary['items']:\n",
    "   id = item['resource_id']\n",
    "   image = f'image-collection-output/image_{id}.jpg'\n",
    "   process_image(image,'../ui/data', item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done! Model outputs stored as JSON and masks/extractions saved to UI\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(data_directory,f\"model_results.json\"), 'w') as f:\n",
    "        json.dump(model_dictionary, f, indent=4)\n",
    "\n",
    "print('All done! Model outputs stored as JSON and masks/extractions saved to UI')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
